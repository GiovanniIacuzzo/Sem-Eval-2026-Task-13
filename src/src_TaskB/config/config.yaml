# -----------------------------------------------------------------------------
# COMMON CONFIGURATION
# -----------------------------------------------------------------------------
common:
  data_dir: "data/Task_B_Processed"
  output_dir: "results/results_TaskB"
  checkpoint_dir: "results/results_TaskB/checkpoints"
  log_dir: "results/results_TaskB/logs"  
  seed: 42
  fp16: true
  num_workers: 4
  num_extra_features: 5   
  learning_rate: 2.0e-5
  warmup_ratio: 0.1
  weight_decay: 0.01
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0  
  languages: ["python", "java", "cpp", "c", "cs", "javascript", "php", "ruby", "rust", "go", "typescript", "kotlin", "swift", "shell"]

# -----------------------------------------------------------------------------
# MODE 1: BINARY (Gatekeeper)
# Task: Human (0) vs AI (1)
# -----------------------------------------------------------------------------
binary:
  model_name: "microsoft/unixcoder-base"
  num_labels: 2
  use_lora: false
  max_length: 512
  batch_size: 24
  gradient_accumulation_steps: 2
  num_epochs: 6
  early_stop_patience: 3  
  class_weights: false
  use_focal_loss: true
  focal_gamma: 2.0
  loss_weights:
    w_ce: 1.0
    w_supcon: 0.15
    w_dann: 0.1

# -----------------------------------------------------------------------------
# MODE 2: FAMILIES (Specialist)
# Task: Classify specific AI model
# Strategy: LoRA per evitare overfitting su classi rare
# -----------------------------------------------------------------------------
families:
  model_name: "microsoft/unixcoder-base"
  num_labels: 10
  use_lora: true
  lora_r: 64
  lora_alpha: 128
  lora_dropout: 0.1
  target_modules: ["query", "value", "key", "dense", "fc", "out_proj"]
  max_length: 512
  batch_size: 32
  gradient_accumulation_steps: 2
  num_epochs: 15
  early_stop_patience: 4
  learning_rate: 4.0e-4
  class_weights: false
  use_focal_loss: true
  focal_gamma: 2.5
  loss_weights:
    w_ce: 1.0
    w_supcon: 0.2
    w_dann: 0.1
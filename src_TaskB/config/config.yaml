# ---------------------
# Dataset Configuration
# ---------------------
data:
  # Percorsi
  train_path: "data/Task_B/train.parquet"
  val_path: "data/Task_B/validation.parquet"
  
  # CRUCIALE: Aumentato a 512. 
  # Grazie a LoRA e UniXCoder, il Mac M2 può reggerlo. 
  # 256 è troppo poco per capire lo stile di un LLM.
  max_length: 512 
  
  balance_languages: True
  samples_per_group: 3000 # Limita i gruppi troppo grandi (Python) per non biasare il modello
  
  task_type: "multiclass"

# ---------------------
# Model Architecture
# ---------------------
model:
  model_name: "microsoft/unixcoder-base"
  
  # 31 classi (Human + 30 AI)
  num_labels: 31
  
  # --- ATTIVAZIONE LORA (M2 Secret Weapon) ---
  # Abilita il training leggero. Consumo RAM ridotto del 70%.
  use_lora: true 
  
  freeze_layers: 0 # Con LoRA non serve freezare layer, freeza tutto lui in automatico
  extra_dropout: 0.05 # Ridotto perché LoRA ha già il suo dropout interno

# ---------------------
# Training Hyperparameters
# ---------------------
training:
  output_dir: "results_TaskB"
  checkpoint_dir: "./results_TaskB/checkpoints"
  log_dir: "./results_TaskB/logs"
  
  # LoRA converge più lentamente del Full Fine-tuning, servono più epoche.
  num_epochs: 1
  early_stop_patience: 3
  
  # --- OTTIMIZZAZIONE M2 ---
  # Con LoRA, il modello pesa pochissimo in RAM. 
  # Possiamo alzare il batch size fisico a 16.
  # 16 batch * 4 accum = 64 Effective Batch Size (Molto stabile)
  batch_size: 4
  gradient_accumulation_steps: 16
  
  # IMPORTANTE: Con LoRA il Learning Rate deve essere ALTO (1e-4 o 2e-4).
  # Se usi 2e-5 (standard BERT), LoRA non impara nulla.
  learning_rate: 0.0002 
  
  evaluation_metric: "f1"
  best_metric_lower_is_better: false
  
  warmup_ratio: 0.1
  fp16: true # Fondamentale per MPS (Metal Performance Shaders)
  
  # Nota: La Focal Loss è gestita internamente dal modello, 
  # questo parametro label_smoothing serve solo se disabilitiamo Focal Loss in futuro.
  label_smoothing: 0.0 

# ---------------------
# Debugging
# ---------------------
demo:
  active: true # Metti 'false' per iniziare il training vero!
  fraction: 0.01
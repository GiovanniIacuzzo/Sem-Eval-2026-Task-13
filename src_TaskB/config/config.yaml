# ==========================================
# SemEval 2026 Task 13 - Task B Configuration
# Optimized for NVIDIA T4 (16GB VRAM) + CUDA
# ==========================================

# ---------------------
# Dataset Configuration
# ---------------------
data:
  # Percorsi
  train_path: "data/Task_B/train.parquet"
  val_path: "data/Task_B/validation.parquet" 
  
  # Context Length
  # 512 è lo sweet spot per UniXCoder.
  # Sulla T4 con LoRA potremmo osare di più, ma 512 garantisce stabilità
  # e copre la maggior parte delle "firme" stilistiche.
  max_length: 512 
  
  # Data Balancing & Split
  # True: evita che Python/Java dominino il training.
  balance_languages: True
  samples_per_group: 4000 
  
  task_type: "multiclass"

# ---------------------
# Model Architecture
# ---------------------
model:
  model_name: "microsoft/unixcoder-base"
  
  # 31 classi (Human + 30 AI Families)
  num_labels: 31
  
  # --- LORA CONFIGURATION ---
  # Attivato nel codice (model.py) con r=64 e alpha=128
  use_lora: true 
  
  # Dropout specifico per il classificatore
  # Lo teniamo basso perché usiamo già Label Smoothing e LoRA Dropout
  extra_dropout: 0.1 

# ---------------------
# Training Hyperparameters
# ---------------------
training:
  output_dir: "results_TaskB"
  checkpoint_dir: "./results_TaskB/checkpoints"
  log_dir: "./results_TaskB/logs"
  
  # --- STRATEGIA DI TRAINING ---
  # LoRA richiede più epoche del full fine-tuning per convergere bene,
  # specialmente con il rank alto (r=64).
  num_epochs: 10
  early_stop_patience: 3
  
  # --- MEMORY TUNING (T4 16GB) ---
  # Con fp16 e LoRA, UniXCoder occupa circa 8-10GB con batch=16.
  # Batch size fisico: 16
  # Accumulation: 4
  # Batch size effettivo: 16 * 4 = 64 (Ideale per stabilità dei gradienti)
  batch_size: 16
  gradient_accumulation_steps: 4
  
  # --- LEARNING RATE ---
  # Per LoRA serve un LR più aggressivo rispetto al full fine-tuning (solitamente 2e-5).
  # 3e-4 è ottimo per AdamW + OneCycleLR scheduler.
  learning_rate: 0.0003
  
  # Mixed Precision (CRITICO per T4)
  # Velocizza il training di 2x-3x e riduce la VRAM.
  fp16: true 
  
  # Metrica per il salvataggio del modello migliore
  evaluation_metric: "f1"
  
  # Label Smoothing è gestito nella Loss Custom, 
  # ma lo lasciamo qui per riferimento futuro.
  label_smoothing: 0.1 

# ---------------------
# Debugging / Demo
# ---------------------
demo:
  # IMPOSTA A 'false' PER IL TRAINING COMPLETO
  # Se 'true', usa solo 2000 righe per testare che il codice non crashi.
  active: false 
  fraction: 0.05
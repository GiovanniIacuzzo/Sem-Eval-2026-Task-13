# üèÜ SemEval-2026 Task 13: GenAI Code Detection & Attribution

<div align="center">
  <a href="README.it.md">
    <img src="https://img.shields.io/badge/Lingua-Italiano-008C45?style=for-the-badge&logo=italian&logoColor=white" alt="Leggi in Italiano">
  </a>
</div>

<br>

This repository contains the unified solution for **SemEval-2026 Task 13**, an international competition focused on the distinction, attribution, and analysis of source code generated by Large Language Models (LLMs) versus human-written code.

The project is structured in a modular way to address the **3 Subtasks** of the competition, sharing a common execution environment and a centralized data analysis pipeline.

---

## üìå Subtasks Overview

The project is divided into three main modules, each with specific objectives and architectures. Click on the task name to access detailed documentation.

| Task | Name | Objective | Problem Type |
| :--- | :--- | :--- | :--- |
| **[Subtask A](src/src_TaskA/README.md)** | **Machine-Generated Code Detection** | Distinguish whether code is written by a Human or a Machine. | *Binary Classification* |
| **[Subtask B](src/src_TaskB/README.md)** | **Multi-Class Authorship Detection** | Identify the specific author model (e.g., GPT-4, Llama-3). | *Multi-class Classification* |
| **[Subtask C](src/src_TaskC/README.md)** | **Mixed-Source Analysis** | Analyze modifications, refactoring, and hybrid Human/AI code. | *Regression / Hybrid* |

---

## üìÇ Repository Structure

The folder organization is designed to separate data, analysis visuals, and source code.

```bash
.
‚îú‚îÄ‚îÄ üìÅ data/                    # Datasets (parquet) split by Task
‚îú‚îÄ‚îÄ üìÅ img/                     # Visual outputs from analysis scripts (EDA)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ img_TaskA/           # Plots specific to Task A
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ img_TaskB/           # Plots specific to Task B
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ img_TaskC/           # Plots specific to Task C
‚îÇ
‚îú‚îÄ‚îÄ üìÅ info_dataset/            # Scripts for statistical data analysis
‚îÇ   ‚îú‚îÄ‚îÄ üêç info_dataset_subTaskA.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç info_dataset_subTaskB.py
‚îÇ   ‚îî‚îÄ‚îÄ üêç info_dataset_subtaskC.py
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/                     # Model source code
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ src_TaskA/           # Complete pipeline for Subtask A
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ src_TaskB/           # Complete pipeline for Subtask B
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ src_TaskC/           # Complete pipeline for Subtask C
‚îÇ
‚îú‚îÄ‚îÄ üêç data.py                  # Downloads the chosen dataset from Kaggle
‚îÇ
‚îú‚îÄ‚îÄ üìù README.md
‚îú‚îÄ‚îÄ üìÑ prepare.sh               # Setup automation script (folder creation & env)
‚îú‚îÄ‚îÄ ‚öôÔ∏è environment.yml          # Shared Conda dependencies
‚îî‚îÄ‚îÄ ‚öôÔ∏è .env                     # Environment variables (generated by prepare.sh)
```


> [!IMPORTANT]
> Remember to generate the `kaggle.json` file from your Kaggle account:
>
> ```bash
> {"username":"your_username","key":"your_kaggle_key"}
> ```
> 

---

## üöÄ Quick Installation Guide

Since the three tasks share the same base dependencies and structure, a centralized setup has been prepared to facilitate startup.

### 1. Prerequisites
* **Anaconda** or **Miniconda** installed on your system.
* An NVIDIA GPU with updated drivers (recommended for training).
* Linux/Mac OS (or WSL for Windows) to run bash scripts.

### 2. Automatic Setup
Run the `prepare.sh` script from the project root. This script will:
1.  Create the output directory structure (`results`, `checkpoints`, etc.).
2.  Generate the `.env` file for environment variables.
3.  Create and install the Conda virtual environment defined in `environment.yml`.

```bash
chmod +x prepare.sh
./prepare.sh
```

### 3. Activating the Environment

Once the setup is complete, activate the environment:
```bash
conda activate semeval
```

### 4. Configurazione dei Dati

Open the `.env` file generated in the project root. Ensure that the `DATA_PATH` variable points to the directory containing the `.parquet` files (or the folder downloaded from Kaggle).

Example `.env`:
```bash
KAGGLE_USERNAME=Your_kaggle_username
KAGGLE_KEY=Your_kaggle_key

DATA_PATH=./data
IMG_PATH=./img

COMET_API_KEY=comet_api_key
COMET_PROJECT_NAME=comet_project_name
COMET_WORKSPACE=comet_name_workspace
COMET_EXPERIMENT_NAME=comet_experment_name
```

### 5. Download Dataset

Remember to install Kaggle dependencies if you haven't already:
```bash
pip install kaggle
```

Download your preferred dataset by running:
```bash
python data.py
```
Edit `competition_name` to insert the specific dataset you wish to download from Kaggle. The dataset will be automatically downloaded into the `data` folder.

---

<div align="center">
  <h2>‚ú® Autore ‚ú®</h2>

  <p>
    <strong>Giovanni Giuseppe Iacuzzo</strong><br>
    <em>AI & Cybersecurity Engineering Student</em><br>
    <em>University of Kore, Enna</em>
  </p>

  <p>
    <a href="https://github.com/giovanniIacuzzo" target="_blank">
      <img src="https://img.shields.io/badge/GitHub-GiovanniIacuzzo-181717?style=for-the-badge&logo=github" alt="GitHub"/>
    </a>
    <a href="mailto:giovanni.iacuzzo@unikorestudent.com">
      <img src="https://img.shields.io/badge/Email-Contattami-D14836?style=for-the-badge&logo=gmail&logoColor=white" alt="Email"/>
    </a>
  </p>
</div>